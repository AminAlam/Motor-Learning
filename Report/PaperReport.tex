\documentclass[9pt,twocolumn]{paper-template}
% Use the lineno option to display guide line numbers if required.
\usepackage{lipsum}
\usepackage{tabularx} % in the preamble
\usepackage{subcaption}
\usepackage{multirow}
\templatetype{twocolumn} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{A Review on Interactive Adaptive Processes Which Underline Short-Term Motor Learning}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a]{MohammadAmin Alamalhoda}
\author[a]{Arsalan Firoozi} 
\author[a]{Mehran khorshidi}
\affil[a]{Student, EE Department, Sharif University of Technology}

% Please add here a significance statement to explain the relevance of your work
\significancestatement{Hmmmmm}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Author contributions}
\equalauthors{\textsuperscript{1}All contributed equally to this work}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
\keywords{Motor learning} 

\begin{abstract}
Abstract
\end{abstract}

\dates{This manuscript was compiled on \today}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{N}ull
\\
\section*{Results}


\textbf{Motor Adaptation Experiments That Show Savings}
Figure \ref{fig:saving} shows simulations of the experimental paradigm that shows savings in eye saccade adaptation. The progress of motor output in learn-unlearn-relearn paradigms with three different models is simulated. Models are (1) a single-state, single time-constant model, (2) a two-state, gain- specific model, and (3) a two-state, gain-independent, multi- rate model. All of these models give motor output as a function of current motor state and the error of last trial. The learning rules for these models are:

\begin{eqnarray*}
& (1)\;Single\;State\;Model:\\
& x(n+1) = Ax(n)+Be(n)\\
\end{eqnarray*}

\begin{eqnarray*}
& (1)\;Gain\;Specific:\\
&x1(n+1) = min(0,[Ax_1(n)+Be(n)])\\
&x2(n+1) = max(0,[Ax_2(n)+Be(n)])\\
&x = x_1+x_2
\end{eqnarray*}


\begin{eqnarray*}
& (1)\;Multi\;Rate:\\
& x1(n+1) = A_fx_1(n) + B_fe(n)\\
& x2(n+1) = A_sx_2(n) + B_se(n)\\
&x = x_1+x_2
\end{eqnarray*}

All of these models have error term ($e(n)$) because there is a difference between the motor output $x(n)$ and the state of the environment $f(n)$, so: $e(n) = f (n) - x(n)$.\\
As can be seen in the results of the simulations, single-state model can't reproduce saving, while both of the gain-specific model proposed by Kojima et al. and multi-rate model proposed by Shadmehr et al. can produce saving. Also, both of these models show decay in the amount of saving when null trials are inserted before the learning block which makes sense based on our intuition of the motor control processes. Because the internal states are different, both systemsâ€™ responses to the learning stimulus are altered. Relearning is faster than initial learning in the gain-specific model because both the up and down states can contribute to relearning whereas only the up state contributes to initial learning. In the case of the multi-rate model, relearning is faster than initial learning because when relearning starts, the
slow state is already biased towards relearning, making relearning more dependent on the fast state compared to initial learning.\\



\textbf{Robustness of spontaneous rebound in the multi-rate model}\\
A key feature of the multi-rate model is its ability to predict the spontaneous recovery. Figure \ref{fig:multi_rate_recovery} only shows spontaneous recovery for
specific sets of model parameters, however spontaneous recovery is a general feature of this
model over a very wide space of model parameters. Because an analytical approach to
demonstrating this property is difficult, we performed a large set of simulations in which the
model parameters where systematically varied (by as much as a factor of 10). In these simulations the fractional spontaneous
recovery (max rebound/max initial learning) was assessed following asymptotic learning and
unlearning to baseline. The results of these simulations are shown in Figure \ref{fig:mutli_rate_params}. There are four
parameters in the multi-rate model and each panel below displays the amount of
spontaneous recovery when two of these parameters are systemically varied. There are six
panels because there are six different two-parameter combinations. Note that in all cases
more than 80\% of the parameter space shown displays a spontaneous recovery of greater
than 20\%, where the amount of spontaneous recovery refers to the ratio of the maximum
recovery in the error clamp phase to the asymptotic amount of learning during the initial
learning phase. These simulations show that spontaneous recovery is a robust feature of
the multi-rate model in this experimental paradigm, and that the finding of spontaneous
recovery does not depend upon a narrow choice of parameter values.\\

\textbf{Memory of errors}\\

As Shadmeher et al. stated in their paper(\cite{mem_of_error}), The brain learns more from the error when it is consistent in time. This is implemented in their model by $\eta$ which is error-sensitivity parameter. They assumed the action $u(n)$ to be zeroSo, the model becomes:

\begin{eqnarray*}
& e(n) = x_n-\hat{x}_n\\
& \hat{x}_{n+1} = \alpha \hat{x}_n + \eta_ne_n
\end{eqnarray*}
where
\begin{eqnarray*}
& \eta(e^{(n)}) = \sum_i^N \omega_ig_i(e^{(n)})\\
&g_i(e^{(n)}) = exp(\frac{-(e^{(n)}-{e^\smallsmile}_i)^2}{2\sigma^2})
\end{eqnarray*}


On trial $n-1$ the motor command $u(n-1)$ produces an error $e(n-1)$ , as illustrated in the top part of Fig. ---. The nervous system learns from this error and produces motor command $u (n)$ on the subsequent trial, resulting in $e(n)$ . In a slowly switching environment (top part of Fig. ---), $e(n)$ has the same sign as $e(n-1)$ . In this case, error-sensitivity should increase around $e(n-1)$ (Fig. ---). On the other hand, in a rapidly switching environment (Fig. ---), $e(n)$ has a different sign than $e(n-1)$ . In this case, error-sensitivity should decrease:

\begin{eqnarray*}
& \omega^{(n+1)} =omega^{(n)}+\beta sign(e^{(n-1)}e^{(n)}) \frac{g(e^{(n-1)})}{g^T(e^{(n-1)})g(e^{(n-1)})}\\
&g_i(e^{(n)}) = exp(\frac{-(e^{(n)}-{e^\smallsmile}_i)^2}{2\sigma^2})
\end{eqnarray*}

We have simulated this model and the results are shown in Figure \ref{fig:herzfeld}




\begin{figure*}[h!]
  \centering
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/single_state_adaptation}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/gain_specific_adaptation}
  \end{subfigure}
   \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/multi_rate_adaptation}
  \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/single_state_relearning}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/gain_specific_relearning}
  \end{subfigure}
   \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/multi_rate_relearning}
  \end{subfigure}
      \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/single_state_saving}
    \caption{Single State}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/gain_specific_saving}
        \caption{Gain Specific}
  \end{subfigure}
   \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure1/multi_rate_saving}
        \caption{Multi Rate}
  \end{subfigure}
  \caption{\textbf{Simulations of Motor Adaptation Experiments That Show Savings}\\
  {First row} shows the model simulations of the experiment paradigm (Disturbance plot) which is plotted in black. {Second row} shows a direct comparison of simulated performance in the initial learning and relearning blocks.  {Third row} shows the amount of savings found in simulation, as a function of the number of washout trials. The amount of savings is measured as the percent improvement in performance on the 30th trial in the relearning block compared to the 30th trial in the initial learning block. 
}
  \label{fig:saving}
\end{figure*}



\begin{figure*}[h!]
  \centering
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure5/Af_AS}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure5/Af_Bf}
  \end{subfigure}
   \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure5/Af_Bs}
  \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure5/As_Bf}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure5/As_Bs}
  \end{subfigure}
   \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/figure5/Bf_Bs}
  \end{subfigure}
  \caption{\textbf{Simulation of the effects of different learning rates and forgetting factors on the amount of the spontaneous rebound predicted by the multi-rate model}\\
  All of the plots show value of the spontaneous rebound vs different parameters of the multi-rate model. default value of the parameters is: $A_f=0.92$, $A_s=0.996$, $B_f=0.03$, and $B_s=0.004$. The amount of the spontaneous rebound is measured as ratio of maximum recovery in the error-clamp phase to the asymptotic amount of learning during the initial learning phase. 
}
  \label{fig:mutli_rate_params}
\end{figure*}


\begin{figure*}[h!]
  \centering
  \begin{subfigure}[b]{0.9\linewidth}
    \includegraphics[width=\linewidth]{figures/figure4/z0_1}
  \end{subfigure}
  \begin{subfigure}[b]{0.9\linewidth}
    \includegraphics[width=\linewidth]{figures/figure4/z0_5}
  \end{subfigure}
   \begin{subfigure}[b]{0.9\linewidth}
    \includegraphics[width=\linewidth]{figures/figure4/z0_9}
  \end{subfigure}
    \begin{subfigure}[b]{0.9\linewidth}
    \includegraphics[width=\linewidth]{figures/figure4/erros_sensivity}
  \end{subfigure}
  \caption{\textbf{Herzfeld Theoretical model}\\
  {First three rows} presents model performance for slow, medium, and rapidly switching environments (gray line represents $\hat{x}^{(n)}$. {Forth row} shows the error-sensivity value over the trials for different values of Z. Bigger error-sensivity values lead to less learning from the error, so model learns more from slow switching environments in comparison with rapidly switching environments.
}
  \label{fig:herzfeld}
\end{figure*}


\newpage

\acknow{We highly appreciate ... }

\showacknow{} % Display the acknowledgments section

\section*{References}
% Bibliography
\bibliography{references}

\bigskip
\begin{center}
All codes are available at \href{https://github.com/MohammadAminAlamalhoda/Motor-Learning}{this link}.
\end{center}
\end{document}